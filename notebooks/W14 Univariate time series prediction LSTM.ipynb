{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3f33679",
   "metadata": {},
   "source": [
    "## This example presents a simple univariate time series prediction problem.\n",
    "\n",
    "We are predicting the next day's Amazon's stock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34ac2672",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e94e056",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19da6bc",
   "metadata": {},
   "source": [
    "Read the amazon stock historical data. We will be using the column 'Close' for predicting the next day's close price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9160dd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "amzn = pd.read_csv(\"https://raw.githubusercontent.com/Ziqi-Li/GIS5122/main/data/AMZN.csv\",date_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b913dcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume'], dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amzn.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a791fbd",
   "metadata": {},
   "source": [
    "### Data preprocessing for LSTM\n",
    "\n",
    "1. Apply a scaler to the stock price to squash its value.\n",
    "2. Define a past datestamp `n_past_steps`, i.e. how many days you want to look back for predicting the next day. Can be a hyperparameter to tune.\n",
    "3. create a sequence of the data, this will eseentially create `n_past_steps` new columns shifted by 1 day, 2 days, 3-days...so on. So you end up with new columns with data t, t-1, t-2.... So these will be used as additional features to predict the next day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0b5d2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Assuming the first column in your features will be your label\n",
    "features = amzn[['Close']] \n",
    "target = amzn[['Close']] \n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(features)\n",
    "target_normalized = scaler.fit_transform(target)\n",
    "\n",
    "n_past_steps = 7  # Number of past timesteps you want to use for prediction\n",
    "\n",
    "# A helper function to prepare data with shifted sequences\n",
    "def create_sequences(data, n_past_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(n_past_steps, len(data)):\n",
    "        X.append(data[i-n_past_steps:i, :])\n",
    "        y.append(data[i, 0])  \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = create_sequences(scaled_data, n_past_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd2b0e8",
   "metadata": {},
   "source": [
    "The shape your feature matrix should be `(n,n_past_steps,k)` where k is 1 for univariate prediction, and k>1 for multivariate prediction.\n",
    "\n",
    "In our case, we have 6509 observations, 7 days to look back and 1 predictor (just the price itself no other variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11a04bad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6509, 7, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d010a1a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6509,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87521d03",
   "metadata": {},
   "source": [
    "Split your data into 80 for training and 20 for test, remember for time-series ordered data, we want to use historical data to train and test on future data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f50f5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "# Splitting dataset into training and testing sets\n",
    "split = int(0.8 * len(X))  # 80% for training, 20% for testing\n",
    "\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d48ec2d",
   "metadata": {},
   "source": [
    "Put everything into tensors and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e3575f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.Tensor(X_train)  # Convert features to a torch Tensor\n",
    "y_train_tensor = torch.Tensor(y_train)  # Convert labels to a torch Tensor\n",
    "X_test_tensor = torch.Tensor(X_test)  # Convert features to a torch Tensor\n",
    "y_test_tensor = torch.Tensor(y_test)  # Convert labels to a torch Tensor\n",
    "\n",
    "# Create a TensorDataset\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d4b79e",
   "metadata": {},
   "source": [
    "Define our LSTM model. The LSTM model will take three parameters:\n",
    "- `input_size`: how many features in have in your model\n",
    "- `hidden_size`: size of your short-term memory (hyper-parameter to tune)\n",
    "- `num_layers` (default 1): allow to stacked LSTM layers, the more the complictated your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b3039b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultivariateLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(MultivariateLSTM, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        #create your LSTM\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        #After LSTM to a linear prediction to get the predicted value\n",
    "        self.linear = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        # Initialize cell state\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        \n",
    "        #Go through your LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Get the last time stamp prediction from LSTM\n",
    "        out = self.linear(out[:, -1, :]) \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e24770",
   "metadata": {},
   "source": [
    "Initialize your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4517cbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultivariateLSTM(input_size=X.shape[-1], hidden_size=32, num_layers=1)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60acec7",
   "metadata": {},
   "source": [
    "Train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e2f671",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ziqili/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/ziqili/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([23])) that is different to the input size (torch.Size([23, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/ziqili/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([22])) that is different to the input size (torch.Size([22, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "#Number of epochs\n",
    "#epochs = 20\n",
    "epochs = 300\n",
    "for epoch in range(epochs):\n",
    "    #turn on training mode\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    #For each batch in our training dataset\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        #reset gradients\n",
    "        optimizer.zero_grad()\n",
    "        #forward to get prediction\n",
    "        y_pred = model(X_batch)\n",
    "        #calculate loss\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        #backward to update weights/biases\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #sum over loss for all the batches.\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    #store the average loss\n",
    "    train_losses.append(train_loss / len(train_loader))\n",
    "\n",
    "    \n",
    "    #turn on evaluation mode\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            #forward to get predicton\n",
    "            y_pred = model(X_batch)\n",
    "            #calculate loss on test data\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            val_loss += loss.item()\n",
    "    #store the average loss\n",
    "    val_losses.append(val_loss / len(test_loader))\n",
    "\n",
    "    #print(f\"Epoch {epoch+1}, Train Loss: {train_losses[-1]}, Val Loss: {val_losses[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c954f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c30178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Test Loss')\n",
    "plt.title('Loss Curves')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "#plt.xlim(0,200)\n",
    "#plt.ylim(0,0.04)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39fb71e",
   "metadata": {},
   "source": [
    "With 300 epochs, you will find the model will overfit after ~20 epochs. The test error will explode. This is an indication you should reduce the number of epochs.\n",
    "\n",
    "Try run the example with 20 epochs to see the results (need to rerun both the Initialize your model code cell and the train model code cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c336caa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1208e1f",
   "metadata": {},
   "source": [
    "Get prediction for both training data and test data to evaluate performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c979fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "predictions = []\n",
    "actuals = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in train_loader:\n",
    "        outputs = model(inputs)\n",
    "        # Store predictions and actual values\n",
    "        predictions.extend(outputs.view(-1).tolist())\n",
    "        \n",
    "        actuals.extend(labels.view(-1).tolist())\n",
    "        \n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        # Store predictions and actual values\n",
    "        predictions.extend(outputs.view(-1).tolist())\n",
    "        \n",
    "        actuals.extend(labels.view(-1).tolist())\n",
    "\n",
    "# Optionally, if you normalized your target variable, you should inverse transform the predictions and actuals here\n",
    "predictions = scaler.inverse_transform(np.array(predictions).reshape(-1, 1)).reshape(-1)\n",
    "actuals = scaler.inverse_transform(np.array(actuals).reshape(-1, 1)).reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362477f3",
   "metadata": {},
   "source": [
    "Plot the predicted price, before the blue line are the training data, and after the blue line is the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5f621b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(actuals,label=\"actual\",alpha=0.5)\n",
    "plt.axvline(x = split, color = 'b')\n",
    "plt.plot(predictions,label=\"predicted\")\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('AMZN price at close')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb55bb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
