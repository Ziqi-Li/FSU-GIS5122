{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression - model specification\n",
    "This notebook demostrates:\n",
    "\n",
    "0. Correctly specified model (baseline)\n",
    "1. Under-specified model\n",
    "    - Uncorrelated predictors\n",
    "    - Correlated predictors\n",
    "2. Over-specified model\n",
    "    - Perfect multi-colinearity\n",
    "    - Varying degrees of Multi-colinearity\n",
    "    - Including non related variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Correctly specified model (baseline)\n",
    "\n",
    "- We randomly generate 100 uncorrelated X1 and X2 following a normal distirbution.\n",
    "\n",
    "- We speficy a true regression line: $y$ = 3 + 2*$X_1$ + 4*$X_2$ + e\n",
    "\n",
    "- We correctly include X1 and X2 in our regression model: y ~ X1 + X2 (with intercept)\n",
    "\n",
    "- We fit 1000 such regression models and check for biasedness and preciseness of the estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimates = []\n",
    "for i in range(1000):\n",
    "    X1 = np.random.randn(100) + 2\n",
    "    X2 = np.random.randn(100) + 2\n",
    "    \n",
    "    e = np.random.randn(100)\n",
    "    y = 3 + 2*X1 + 4*X2 + e\n",
    "    X = sm.add_constant(np.column_stack([X1,X2]))\n",
    "    model = sm.OLS(y, X).fit()\n",
    "    estimates.append(model.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average of estimates: [3.00195035 2.00115695 3.99953601]\n",
      "SE of estimates: [0.29882325 0.10161407 0.10566198]\n"
     ]
    }
   ],
   "source": [
    "#The averages of estimates across the 1000 replications are:\n",
    "print(\"Average of estimates:\", np.mean(np.array(estimates),axis=0))\n",
    "print(\"SE of estimates:\", np.std(np.array(estimates),axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Under-specified model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uncorrelated predictors (#1)\n",
    "- We randomly generate 100 uncorrelated X1 and X2 following a normal distirbution.\n",
    "\n",
    "- We speficy a true regression line: $y$ = 3 + 2*$X_1$ + 4*$X_2$ + e\n",
    "\n",
    "- We only include X1 in our regression model: y ~ X1 (with intercept)\n",
    "\n",
    "- We fit 1000 such regression models and check for biasedness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimates = []\n",
    "for i in range(1000):\n",
    "    means = [2 , 2]\n",
    "    cov = [[1,0], [0,1]]\n",
    "    \n",
    "    X = np.random.multivariate_normal(means,cov,100)\n",
    "\n",
    "    e = np.random.randn(100)\n",
    "    \n",
    "    y = 3 + 2*X[:,0] + 4*X[:,1] + e\n",
    "    \n",
    "    X = sm.add_constant(X[:,0])\n",
    "    model = sm.OLS(y, X).fit()\n",
    "    \n",
    "    estimates.append(model.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average of estimates: [11.00273446  1.99328831]\n",
      "SE of estimates: [0.94906234 0.42030678]\n"
     ]
    }
   ],
   "source": [
    "#The averages of estimates across the 1000 replications are:\n",
    "print(\"Average of estimates:\", np.mean(np.array(estimates),axis=0))\n",
    "\n",
    "print(\"SE of estimates:\", np.std(np.array(estimates),axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the intercept estimate is **biased** (if will be unbiased only if your omitted variable has a mean of zero), and the coefficient for b1 is **unbiased**. The SE of the estimates are much higher than that in a correctly specified model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlated predictors (#2)\n",
    "- We randomly generate 100 correlated X1 and X2 following a bi-variate normal distirbution (so X1 and X2 are having a correlation coefficient of 0.4).\n",
    "\n",
    "- We speficy a true regression line: $y$ = 3 + 2*$X_1$ + 4*$X_2$ + e\n",
    "\n",
    "- We only include X1 in our regression model: y ~ X1 (with intercept)\n",
    "\n",
    "- We fit 1000 such regression models and check for biasedness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimates = []\n",
    "for i in range(1000):\n",
    "    means = [2, 2]\n",
    "    cov = [[1,0.4], [0.4,1]]\n",
    "    X = np.random.multivariate_normal(means,cov,100)\n",
    "\n",
    "    e = np.random.randn(100)\n",
    "    y = 3 + 2*X[:,0] + 4*X[:,1] + e\n",
    "    X = sm.add_constant(X[:,0])\n",
    "    model = sm.OLS(y, X).fit()\n",
    "    estimates.append(model.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.78186818, 3.60331787])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The averages of estimates across the 1000 replications are:\n",
    "np.mean(np.array(estimates),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.86955328, 0.39256739])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The averages of estimates across the 1000 replications are:\n",
    "np.std(np.array(estimates),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the intercept estimate is **biased** (it will be unbiased only if both of your variable have a mean of zero), and the coefficient for b1 is also **biased** from its true value of 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: when predictors are correlated, omitting one will make other estimates biased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Over-specified model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perfect multicolinearity (#1)\n",
    "- We randomly generate 100 data points for X1 following a normal distribution.\n",
    "- We set X2 = 1 - X1\n",
    "\n",
    "- We speficy a true regression line: $y$ = 3 + 2*$X_1$ + 4*$X_2$ + e\n",
    "\n",
    "- We only fit the regression model as: y ~ X1 + X2 (with intercept)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = np.random.randn(100)\n",
    "X2 = 1 - X1\n",
    "e = np.random.randn(100)\n",
    "\n",
    "y = 3 + 2*X1 + 4*X2 + e\n",
    "\n",
    "X = sm.add_constant(np.column_stack([X1,X2]))\n",
    "\n",
    "model = sm.OLS(y,X).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.847</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.846</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   543.1</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 29 Jan 2025</td> <th>  Prob (F-statistic):</th> <td>9.39e-42</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>21:42:42</td>     <th>  Log-Likelihood:    </th> <td> -137.42</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   100</td>      <th>  AIC:               </th> <td>   278.8</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    98</td>      <th>  BIC:               </th> <td>   284.1</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    3.9652</td> <td>    0.069</td> <td>   57.135</td> <td> 0.000</td> <td>    3.827</td> <td>    4.103</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    0.9980</td> <td>    0.064</td> <td>   15.620</td> <td> 0.000</td> <td>    0.871</td> <td>    1.125</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>    2.9672</td> <td>    0.044</td> <td>   68.140</td> <td> 0.000</td> <td>    2.881</td> <td>    3.054</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 0.767</td> <th>  Durbin-Watson:     </th> <td>   2.198</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.682</td> <th>  Jarque-Bera (JB):  </th> <td>   0.392</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.124</td> <th>  Prob(JB):          </th> <td>   0.822</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.180</td> <th>  Cond. No.          </th> <td>1.14e+16</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The smallest eigenvalue is 2.61e-30. This might indicate that there are<br/>strong multicollinearity problems or that the design matrix is singular."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &        y         & \\textbf{  R-squared:         } &     0.847   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.846   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     543.1   \\\\\n",
       "\\textbf{Date:}             & Wed, 29 Jan 2025 & \\textbf{  Prob (F-statistic):} &  9.39e-42   \\\\\n",
       "\\textbf{Time:}             &     21:42:42     & \\textbf{  Log-Likelihood:    } &   -137.42   \\\\\n",
       "\\textbf{No. Observations:} &         100      & \\textbf{  AIC:               } &     278.8   \\\\\n",
       "\\textbf{Df Residuals:}     &          98      & \\textbf{  BIC:               } &     284.1   \\\\\n",
       "\\textbf{Df Model:}         &           1      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "               & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const} &       3.9652  &        0.069     &    57.135  &         0.000        &        3.827    &        4.103     \\\\\n",
       "\\textbf{x1}    &       0.9980  &        0.064     &    15.620  &         0.000        &        0.871    &        1.125     \\\\\n",
       "\\textbf{x2}    &       2.9672  &        0.044     &    68.140  &         0.000        &        2.881    &        3.054     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       &  0.767 & \\textbf{  Durbin-Watson:     } &    2.198  \\\\\n",
       "\\textbf{Prob(Omnibus):} &  0.682 & \\textbf{  Jarque-Bera (JB):  } &    0.392  \\\\\n",
       "\\textbf{Skew:}          &  0.124 & \\textbf{  Prob(JB):          } &    0.822  \\\\\n",
       "\\textbf{Kurtosis:}      &  3.180 & \\textbf{  Cond. No.          } & 1.14e+16  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The smallest eigenvalue is 2.61e-30. This might indicate that there are \\newline\n",
       " strong multicollinearity problems or that the design matrix is singular."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.847\n",
       "Model:                            OLS   Adj. R-squared:                  0.846\n",
       "Method:                 Least Squares   F-statistic:                     543.1\n",
       "Date:                Wed, 29 Jan 2025   Prob (F-statistic):           9.39e-42\n",
       "Time:                        21:42:42   Log-Likelihood:                -137.42\n",
       "No. Observations:                 100   AIC:                             278.8\n",
       "Df Residuals:                      98   BIC:                             284.1\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          3.9652      0.069     57.135      0.000       3.827       4.103\n",
       "x1             0.9980      0.064     15.620      0.000       0.871       1.125\n",
       "x2             2.9672      0.044     68.140      0.000       2.881       3.054\n",
       "==============================================================================\n",
       "Omnibus:                        0.767   Durbin-Watson:                   2.198\n",
       "Prob(Omnibus):                  0.682   Jarque-Bera (JB):                0.392\n",
       "Skew:                           0.124   Prob(JB):                        0.822\n",
       "Kurtosis:                       3.180   Cond. No.                     1.14e+16\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 2.61e-30. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Multicolinearity (#2)\n",
    "- We randomly generate 100 correlated X1 and X2 following a bi-variate normal distirbution and let X1 and X2 have different degree of correlation (0, 0.3, 0.6, 0.9, 0.95, and 0.99).\n",
    "\n",
    "- We speficy a true regression line: $y$ = 2*$X_1$ + 4*$X_2$ + e\n",
    "\n",
    "- We include X1 and X2 in our regression model: y ~ X1 + X2 (no intercept, for simplicity)\n",
    "\n",
    "- For each pre-set correlation value, we repeat 1000 times to generate the sampling distirbution of regression estimates.\n",
    "- Plot the sampling distirbutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a function to return the sampling distribution of regression coefficients \n",
    "# based on different degree of correlation between X1 and X2\n",
    "def simulation_multi_colinearity(cor):\n",
    "    params = []\n",
    "    for i in range(1000):\n",
    "        means = [2,2]\n",
    "        cov = [[1,cor], [cor,1]]\n",
    "        X = np.random.multivariate_normal(means,cov,100)\n",
    "        e = np.random.randn(100)*2\n",
    "        y = 2*X[:,0] + 4*X[:,1] + e\n",
    "        model = sm.OLS(y,X).fit()\n",
    "        params.append(model.params)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_dist_0 = simulation_multi_colinearity(0)\n",
    "sampling_dist_0_3 = simulation_multi_colinearity(0.3)\n",
    "sampling_dist_0_6 = simulation_multi_colinearity(0.6)\n",
    "sampling_dist_0_9 = simulation_multi_colinearity(0.9)\n",
    "sampling_dist_0_95 = simulation_multi_colinearity(0.95)\n",
    "sampling_dist_0_99 = simulation_multi_colinearity(0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SE of estimates when X1 and X2 have a cor of 0: [0.14975577 0.14674037]\n",
      "SE of estimates when X1 and X2 have a cor of 0.3: [0.17521922 0.17300788]\n",
      "SE of estimates when X1 and X2 have a cor of 0.6: [0.23954783 0.23633053]\n",
      "SE of estimates when X1 and X2 have a cor of 0.9: [0.44319264 0.44149093]\n",
      "SE of estimates when X1 and X2 have a cor of 0.95: [0.64226773 0.64220472]\n",
      "SE of estimates when X1 and X2 have a cor of 0.99: [1.47670598 1.47910517]\n"
     ]
    }
   ],
   "source": [
    "print(\"SE of estimates when X1 and X2 have a cor of 0:\", np.std(sampling_dist_0,axis=0))\n",
    "print(\"SE of estimates when X1 and X2 have a cor of 0.3:\", np.std(sampling_dist_0_3,axis=0))\n",
    "print(\"SE of estimates when X1 and X2 have a cor of 0.6:\", np.std(sampling_dist_0_6,axis=0))\n",
    "print(\"SE of estimates when X1 and X2 have a cor of 0.9:\", np.std(sampling_dist_0_9,axis=0))\n",
    "print(\"SE of estimates when X1 and X2 have a cor of 0.95:\", np.std(sampling_dist_0_95,axis=0))\n",
    "print(\"SE of estimates when X1 and X2 have a cor of 0.99:\", np.std(sampling_dist_0_99,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of estimates when X1 and X2 have a cor of 0: [2.0016277  4.00016232]\n",
      "Mean of estimates when X1 and X2 have a cor of 0.3: [2.00211592 4.00058463]\n",
      "Mean of estimates when X1 and X2 have a cor of 0.6: [2.00172266 4.00223066]\n",
      "Mean of estimates when X1 and X2 have a cor of 0.9: [1.99749542 4.00501842]\n",
      "Mean of estimates when X1 and X2 have a cor of 0.95: [1.99934875 3.99953973]\n",
      "Mean of estimates when X1 and X2 have a cor of 0.99: [1.95444333 4.04792674]\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean of estimates when X1 and X2 have a cor of 0:\", np.mean(sampling_dist_0,axis=0))\n",
    "print(\"Mean of estimates when X1 and X2 have a cor of 0.3:\", np.mean(sampling_dist_0_3,axis=0))\n",
    "print(\"Mean of estimates when X1 and X2 have a cor of 0.6:\", np.mean(sampling_dist_0_6,axis=0))\n",
    "print(\"Mean of estimates when X1 and X2 have a cor of 0.9:\", np.mean(sampling_dist_0_9,axis=0))\n",
    "print(\"Mean of estimates when X1 and X2 have a cor of 0.95:\", np.mean(sampling_dist_0_95,axis=0))\n",
    "print(\"Mean of estimates when X1 and X2 have a cor of 0.99:\", np.mean(sampling_dist_0_99,axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that, with varying degrees of correlation between the two predictors in the model, the regression coefficients are still **unbiased**, but the standard error of the estimates are **inflated** based on the degree of the correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding non-related predictors (#2)\n",
    "- We randomly generate 100 data points for X1, X2, and X3 following a normal distribution.\n",
    "\n",
    "- We speficy a true regression line: $y$ = 3 + 2*$X_1$ + 4*$X_2$ + e\n",
    "\n",
    "- We  include X1, X2, and X3in our regression model: y ~ X1 (with intercept). Here X3 should not be in the model, but included.\n",
    "\n",
    "- We fit 1000 such regression models and check for biasedness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimates = []\n",
    "for i in range(1000):\n",
    "    X1 = np.random.randn(100)\n",
    "    X2 = np.random.randn(100)\n",
    "    X3 = np.random.randn(100)\n",
    "    e = np.random.randn(100)\n",
    "\n",
    "    y = 3 + 2*X1 + 4*X2 + e\n",
    "\n",
    "    X = sm.add_constant(np.column_stack([X1,X2,X3]))\n",
    "    model = sm.OLS(y, X).fit()\n",
    "    estimates.append(model.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average of estimates: [ 3.00559236e+00  2.00282613e+00  4.00539610e+00 -3.54773022e-03]\n",
      "SE of estimates: [0.1031899  0.10409916 0.10071232 0.10225802]\n"
     ]
    }
   ],
   "source": [
    "#The averages of estimates across the 1000 replications are:\n",
    "print(\"Average of estimates:\", np.mean(np.array(estimates),axis=0))\n",
    "print(\"SE of estimates:\", np.std(np.array(estimates),axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find the estimates of X1 and X2 are **unbiased**, and the standard errors are also quite small. The estimate for X3 is nearly **zero**, and if we check one model out of the 1000, we find the estimate is not statistically significant. In this sense, including non-related random data into the model does not do  much harm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.956</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.954</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   692.0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 29 Jan 2025</td> <th>  Prob (F-statistic):</th> <td>7.37e-65</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>21:42:43</td>     <th>  Log-Likelihood:    </th> <td> -133.51</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   100</td>      <th>  AIC:               </th> <td>   275.0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    96</td>      <th>  BIC:               </th> <td>   285.4</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    2.9365</td> <td>    0.095</td> <td>   30.995</td> <td> 0.000</td> <td>    2.748</td> <td>    3.125</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    1.9564</td> <td>    0.104</td> <td>   18.897</td> <td> 0.000</td> <td>    1.751</td> <td>    2.162</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>    4.1883</td> <td>    0.096</td> <td>   43.420</td> <td> 0.000</td> <td>    3.997</td> <td>    4.380</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>    0.0342</td> <td>    0.106</td> <td>    0.323</td> <td> 0.747</td> <td>   -0.176</td> <td>    0.244</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 0.758</td> <th>  Durbin-Watson:     </th> <td>   1.889</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.685</td> <th>  Jarque-Bera (JB):  </th> <td>   0.803</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.010</td> <th>  Prob(JB):          </th> <td>   0.669</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.562</td> <th>  Cond. No.          </th> <td>    1.33</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &        y         & \\textbf{  R-squared:         } &     0.956   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.954   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     692.0   \\\\\n",
       "\\textbf{Date:}             & Wed, 29 Jan 2025 & \\textbf{  Prob (F-statistic):} &  7.37e-65   \\\\\n",
       "\\textbf{Time:}             &     21:42:43     & \\textbf{  Log-Likelihood:    } &   -133.51   \\\\\n",
       "\\textbf{No. Observations:} &         100      & \\textbf{  AIC:               } &     275.0   \\\\\n",
       "\\textbf{Df Residuals:}     &          96      & \\textbf{  BIC:               } &     285.4   \\\\\n",
       "\\textbf{Df Model:}         &           3      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "               & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const} &       2.9365  &        0.095     &    30.995  &         0.000        &        2.748    &        3.125     \\\\\n",
       "\\textbf{x1}    &       1.9564  &        0.104     &    18.897  &         0.000        &        1.751    &        2.162     \\\\\n",
       "\\textbf{x2}    &       4.1883  &        0.096     &    43.420  &         0.000        &        3.997    &        4.380     \\\\\n",
       "\\textbf{x3}    &       0.0342  &        0.106     &     0.323  &         0.747        &       -0.176    &        0.244     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       &  0.758 & \\textbf{  Durbin-Watson:     } &    1.889  \\\\\n",
       "\\textbf{Prob(Omnibus):} &  0.685 & \\textbf{  Jarque-Bera (JB):  } &    0.803  \\\\\n",
       "\\textbf{Skew:}          & -0.010 & \\textbf{  Prob(JB):          } &    0.669  \\\\\n",
       "\\textbf{Kurtosis:}      &  2.562 & \\textbf{  Cond. No.          } &     1.33  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.956\n",
       "Model:                            OLS   Adj. R-squared:                  0.954\n",
       "Method:                 Least Squares   F-statistic:                     692.0\n",
       "Date:                Wed, 29 Jan 2025   Prob (F-statistic):           7.37e-65\n",
       "Time:                        21:42:43   Log-Likelihood:                -133.51\n",
       "No. Observations:                 100   AIC:                             275.0\n",
       "Df Residuals:                      96   BIC:                             285.4\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          2.9365      0.095     30.995      0.000       2.748       3.125\n",
       "x1             1.9564      0.104     18.897      0.000       1.751       2.162\n",
       "x2             4.1883      0.096     43.420      0.000       3.997       4.380\n",
       "x3             0.0342      0.106      0.323      0.747      -0.176       0.244\n",
       "==============================================================================\n",
       "Omnibus:                        0.758   Durbin-Watson:                   1.889\n",
       "Prob(Omnibus):                  0.685   Jarque-Bera (JB):                0.803\n",
       "Skew:                          -0.010   Prob(JB):                        0.669\n",
       "Kurtosis:                       2.562   Cond. No.                         1.33\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
